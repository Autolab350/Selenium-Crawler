SELENIUM CRAWLER PROJECT - PLAIN TEXT DOCUMENTATION

OVERVIEW
This project implements a robust web crawler using Selenium for extracting data from dynamic websites. It includes browser automation, authentication, rate limiting, caching, and data extraction.

CORE PROBLEM
Basic fetch_webpage tool only retrieves static HTML. It cannot:
- Execute JavaScript to load dynamic content
- Handle authentication and login pages
- Navigate multi-page data (pagination)
- Extract structured data from complex layouts

SOLUTION
Selenium opens a real headless browser that:
- Executes JavaScript like a user would
- Fills forms and clicks buttons
- Waits for dynamic content to load
- Manages cookies and sessions
- Extracts live market data exactly as displayed

PROJECT STRUCTURE
selenium_crawler/
├── __init__.py
├── browser_config.py       Browser setup and control
├── auth_manager.py         Authentication and session handling
├── rate_limiter.py         Rate limiting to avoid blocking
├── smart_cache.py          Intelligent caching with TTL
├── data_parser.py          HTML parsing and data extraction
└── crawler.py              Main crawler orchestration

KEY COMPONENTS

1. BROWSER CONFIG
- Headless Chrome setup
- Dynamic content waiting
- JavaScript execution
- User agent spoofing
- Popup/notification blocking

2. AUTH MANAGER
- Login flow automation
- Cookie persistence
- Session management
- Avoids repeated logins

3. RATE LIMITER
- Respects request limits (30 per minute default)
- Enforces delays between requests
- Prevents IP blocking
- Configurable per site

4. SMART CACHE
- TTL-based caching (24 hours default)
- MD5 hash keys for consistency
- Automatic old cache cleanup
- JSON-based storage

5. DATA PARSER
- BeautifulSoup HTML parsing
- JSON extraction from scripts
- Market data extraction
- Structured data output

6. CRAWLER ENGINE
- Orchestrates all components
- Handles pagination
- Manages page fetch lifecycle
- Integrates with agent systems

WORKFLOW
User Request
-> Agent decides to use Selenium Crawler
-> Crawler starts headless browser
-> Fetches page with rate limiting
-> Waits for JavaScript content
-> Parses HTML with BeautifulSoup
-> Extracts structured data
-> Caches results
-> Returns data to agent
-> Agent performs analysis

AGENT INTEGRATION
Once deployed, agents can request:
"Get me all Polymarket election markets with current odds"

Crawler will:
1. Open https://polymarket.com/markets
2. Wait for markets to render
3. Extract: title, id, yes_price, no_price, volume, liquidity
4. Return structured JSON data

DEPENDENCIES
pip install selenium beautifulsoup4 webdriver-manager

USAGE EXAMPLES

Example 1: Simple extraction
```
crawler = SeleniumCrawler()
crawler.start()
markets = crawler.extract_polymarket_markets()
crawler.stop()
```

Example 2: With authentication
```
crawler = SeleniumCrawler()
crawler.start()
if not crawler.auth_manager.load_cookies(crawler.driver, "polymarket"):
    crawler.auth_manager.login_polymarket(crawler.driver, email, password)
markets = crawler.extract_polymarket_markets()
crawler.stop()
```

Example 3: Pagination
```
markets = crawler.handle_pagination(
    base_url="https://polymarket.com/markets",
    page_selector=".market-card",
    next_button_xpath="//button[@aria-label='Next']",
    max_pages=5
)
```

RATE LIMITING STRATEGY
- 30 requests per minute default
- 0.5 second delay between requests
- Automatic backoff when limit approached
- Configurable per crawler instance

CACHING STRATEGY
- 24 hour default TTL
- MD5 hash of URL + params as key
- Automatic expiration
- Manual cleanup available
- Format: JSON with metadata

POLYMARKET EXTRACTION
Extracts from market cards:
- Market title
- Market ID (from URL)
- YES outcome price
- NO outcome price
- Trading volume
- Available liquidity
- Timestamp of extraction

ADVANTAGES OVER BASIC FETCHING
Basic fetch_webpage limitations:
- Gets raw HTML only
- No JavaScript execution
- Cannot interact with forms
- Cannot handle dynamic loading
- No session management

Selenium advantages:
- Real browser rendering
- JavaScript execution
- Form interaction
- Dynamic content waiting
- Session management
- Cookie persistence

PERFORMANCE NOTES
First request: 3-5 seconds (browser startup)
Subsequent requests: 1-2 seconds each
Cached requests: <50ms
Bulk extraction (100 items): 20-30 seconds

FAILURE HANDLING
- Automatic retries on timeout
- Graceful degradation on parse errors
- Cache fallback on failure
- Detailed logging of issues
- Custom exception handling

ERROR SCENARIOS HANDLED
- Page load timeout
- Element not found
- Authentication failures
- Rate limiting
- Network errors
- Malformed HTML
- Missing data fields

STORAGE OPTIONS
Results can be saved as:
- JSON (recommended)
- CSV (via pandas)
- SQL database
- In-memory (for agents)

INTEGRATION WITH AGENT DATA ROUTER
The crawler integrates with the agent data router system:

Register resource:
```
router.register_resource(ResourceAssignment(
    resource_name="live_polymarket_data",
    resource_type="market_data",
    source="selenium_crawler",
    location="https://polymarket.com/markets",
    metadata={"extraction_type": "polymarket_markets", "cache_ttl_hours": 4}
))
```

Agent accesses via:
```
data = router.get_resource("live_polymarket_data")
```

NEXT STEPS
1. Install dependencies
2. Create project structure
3. Copy Python implementations
4. Test with simple page
5. Configure for Polymarket
6. Integrate with agent router
7. Deploy and monitor

SECURITY NOTES
- Use environment variables for credentials
- Never hardcode passwords
- Rotate API keys regularly
- Store cookies in secure location
- Validate extracted data
- Monitor for blocking/banning

SCALING CONSIDERATIONS
- Use thread pool for multiple crawlers
- Implement distributed caching
- Monitor rate limits carefully
- Rotate user agents
- Use proxy rotation if needed
- Implement circuit breaker pattern

MAINTENANCE
- Monitor cache size
- Clean old cache files
- Update selectors if site changes
- Monitor for IP blocking
- Track extraction success rates
- Log failed extractions for analysis
