SELENIUM CRAWLER - INSTALLATION & QUICK START

STEP 1: INSTALL DEPENDENCIES

cd "/home/jack/Documents/Orion/Selenium Crawler"
pip install -r requirements.txt

This installs:
- selenium (browser automation)
- beautifulsoup4 (HTML parsing)
- webdriver-manager (ChromeDriver management)
- pandas (data handling)

STEP 2: TEST INSTALLATION

python example_1_simple.py

Expected output:
- Browser starts
- Extracts Polymarket markets
- Displays 5 market samples
- Saves results to JSON
- Browser stops

STEP 3: USE IN YOUR CODE

from crawler import SeleniumCrawler

# Create crawler
crawler = SeleniumCrawler()

# Start browser
crawler.start()

# Extract data
markets = crawler.extract_polymarket_markets()

# Use data
for market in markets:
    print(f"{market['title']}: {market['yes_price']}")

# Stop browser
crawler.stop()

STEP 4: INTEGRATE WITH AGENT ROUTER

from agent_tools.agent_data_router import AgentDataRouter, ResourceAssignment

router = AgentDataRouter()

router.register_resource(ResourceAssignment(
    resource_name="live_polymarket_data",
    resource_type="market_data",
    source="selenium_crawler",
    location="https://polymarket.com/markets",
    access_key="NONE",
    metadata={"cache_ttl_hours": 4}
))

# Now Claude agents can use:
# data = router.get_resource("live_polymarket_data")

TROUBLESHOOTING

Issue: "No such file or directory: chromedriver"
Fix: pip install --upgrade webdriver-manager

Issue: "element not found"
Fix: Website layout may have changed, update selectors

Issue: Browser times out
Fix: Increase timeout or check internet connection

Issue: Rate limited
Fix: Reduce requests_per_minute in SeleniumCrawler config

NEXT STEPS

1. Review README.txt for detailed docs
2. Check PROJECT_GUIDE.txt for architecture
3. Run all 3 examples
4. Customize for your needs
5. Deploy as agent resource

All files are ready to use. Start with example_1_simple.py
